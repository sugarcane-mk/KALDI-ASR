{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zkcahO3W8J0s"
      ],
      "name": "dataprep_4_asr_kaldi.ipynb",
      "mount_file_id": "1CfRrnB-aYpATBo1DAc3c0XYcjCsLnim5",
      "authorship_tag": "ABX9TyM1/Po7SIbBLsvW2PIfy1+I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarcane-mk/KALDI-ASR/blob/main/dataprep_4_asr_kaldi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u-e6HXdev52z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "id": "iqt_Tg9YjP5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation"
      ],
      "metadata": {
        "id": "zkcahO3W8J0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The audio files are segmented into individual sentences using `.lab` files, which contain the start and end timestamps for each sentence. These timestamps guide the extraction of specific segments from the audio files, creating individual files for each sentence.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wpd_ftD9dIyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pydub\n",
        "from pydub import AudioSegment\n",
        "import IPython.display as ipd\n",
        "with open('/content/drive/Shareddrives/Audio_ processing/Segmented_data/Punitha_FPU/label/478-487/478-487(10).lab','r') as f:  #lab file\n",
        "  lines = f.readlines()\n",
        "\n",
        "lines=lines[1::2]\n",
        "\n",
        "# lines=lines[16::2]\n",
        "# lines=['66275000 85800000 61\\n',\n",
        "#  '104650000 122900057 62\\n',\n",
        "#  '137975000 151775000 63\\n',\n",
        "#  '161975000 175725000 64\\n',\n",
        "#  '187150000 207500000 65\\n',\n",
        "#  '216250000 235299943 66\\n',\n",
        "#  '248850000 265975000 67\\n',\n",
        "#  '278700000 298274943 68\\n',\n",
        "#  '320200000 342550000 69\\n',\n",
        "# '383950000 407500057 70\\n']\n",
        "\n",
        "print('No. of sentences : ', len(lines))\n",
        "\n",
        "lines\n"
      ],
      "metadata": {
        "id": "1NnH186s1q7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import IPython.display as ipd\n",
        "import os\n",
        "import re\n",
        "\n",
        "audio_file = '/content/drive/Shareddrives/Audio_ processing/Segmented_data/Punitha_FPU/label/478-487/478-487(10).wav'   # Audio files path\n",
        "\n",
        "match = re.search(r\"/Segmented_data/([^/]+)/label/([^/]+)/\", audio_file)  # re search to extract speaker and sentence folder name\n",
        "\n",
        "# Speaker and session information\n",
        "speaker_folder = match.group(1)      # speaker folder name\n",
        "sentence_folder = match.group(2)     # sentence folder name\n",
        "speaker = \"FPU\"                      # speaker name\n",
        "session = 10                          # session number\n",
        "\n",
        "\n",
        "for line in lines:\n",
        "  parts = line.strip().split()\n",
        "  start_time = float(parts[0]) / 10000\n",
        "  end_time = float(parts[1]) / 10000\n",
        "  sentence_number = int(parts[2])\n",
        "\n",
        "  audio = AudioSegment.from_file(audio_file)\n",
        "  extracted_audio = audio[start_time:end_time]\n",
        "\n",
        "  output_file = f\"/content/drive/Shareddrives/Audio_ processing/Segmented_data/{speaker_folder}/speech/{sentence_folder}/{speaker}_{sentence_number}_{session}.wav\"   # MSU_1_1.wav # Audio file path\n",
        "  os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "  extracted_audio.export(output_file, format=\"wav\")\n",
        "  # print(f'File saved to path {output_file}')\n",
        "\n",
        "\n",
        "print(f\"Process completed for speaker {speaker_folder} for sentence {sentence_folder}, and session {session}.\")"
      ],
      "metadata": {
        "id": "T2iHwvys5rn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSR\n",
        "*   131-155(7).....142 missing\n",
        "*   131-155(9).....152 missing\n",
        "*   156-180(5).....161 162 missing\n",
        "*   156-180(8).....159-162 164 176 missing\n",
        "*   156-180(9).....160-162 168 176 missing\n",
        "*   181-196-all....186 number issue\n",
        "\n",
        "FPU\n",
        "*  262-285(10).....280 Missing"
      ],
      "metadata": {
        "id": "fdQGkS1FYePV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import IPython.display as ipd\n",
        "import os\n",
        "\n",
        "audio_file = '/content/drive/Shareddrives/Audio_ processing/Segmented_data/MSU/label/156-180/156-180(10).wav'\n",
        "\n",
        "# Speaker and session information\n",
        "speaker = \"MSU\"\n",
        "session = 10\n",
        "\n",
        "\n",
        "for line in lines:\n",
        "  parts = line.strip().split()\n",
        "  start_time = float(parts[0]) / 10000\n",
        "  end_time = float(parts[1]) / 10000\n",
        "  sentence_number = int(parts[2])\n",
        "\n",
        "  audio = AudioSegment.from_file(audio_file)\n",
        "  extracted_audio = audio[start_time:end_time]\n",
        "\n",
        "  output_file = f\"/content/drive/Shareddrives/Audio_ processing/Segmented_data/{speaker}/speech/156-180/{speaker}_{sentence_number}_{session}.wav\"\n",
        "  os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "  extracted_audio.export(output_file, format=\"wav\")\n",
        "\n",
        "\n",
        "print(\"done\")"
      ],
      "metadata": {
        "id": "5ffMoFvjrQ90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create text file"
      ],
      "metadata": {
        "id": "dKBoKxl49u_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I have sentence number and transcript in a txt file eg. 1. kadxawulai wandandgu.\n",
        "# @title Code to create Text file with 10 repetition for each sentence.\n",
        "# @markdown ###Input file:\n",
        "# @markdown - `sentenceid. \\t transcript` ---> `spkid_sentenceid_repeatcount \\t transcript `\n",
        "# @ <speaker_sentence number_repeatcount> \\t transcript each transcript should be repeated 10 times\n",
        "import re\n",
        "\n",
        "def create_spk2utt(input_file, output_file):\n",
        "  \"\"\"\n",
        "  Processes a text file containing sentence numbers and transcripts,\n",
        "  cleans the transcripts, and creates a spk2utt.txt file.\n",
        "\n",
        "  Args:\n",
        "    input_file: Path to the input text file.\n",
        "    output_file: Path to the output spk2utt.txt file.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "      for line in infile:\n",
        "        parts = line.strip().split('.', 1)\n",
        "        if len(parts) == 2:\n",
        "          spk_id = \"FPU\" # @param {\"type\":\"string\",\"placeholder\":\"SpeakerID eg. FPU\"}\n",
        "          sentence_number = parts[0].strip()\n",
        "          transcript = parts[1].strip()\n",
        "\n",
        "          # Clean the transcript\n",
        "          transcript = re.sub(r'[^\\w\\s]', '', transcript)  # Remove punctuation\n",
        "          transcript = transcript.lower()  # Convert to lowercase\n",
        "\n",
        "          # Write to spk2utt.txt with 10 repetitions\n",
        "          for i in range(1,11):\n",
        "            outfile.write(f\"{spk_id}_{sentence_number}_{i}\\t{transcript}\\n\")                   # Modify Speaker ID\n",
        "  except FileNotFoundError:\n",
        "      print(f\"Error: Input file '{input_file}' not found.\")\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage (replace with your actual file paths)\n",
        "input_file_path = \"/content/transctipts_FPU(1).txt\" # @param {\"type\":\"string\",\"placeholder\":\"Path to transcript file\"}\n",
        "output_file_path = \"/content/drive/Shareddrives/Audio_ processing/Segmented_data/Punitha_FPU/text_all\" # @param {\"type\":\"string\",\"placeholder\":\"Path to save uttrances\"}\n",
        "create_spk2utt(input_file_path, output_file_path)\n",
        "\n",
        "print(f'Created text file at {output_file_path}')"
      ],
      "metadata": {
        "id": "HmNKDQGxP10r",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compare and Remove transcripts for missing wav files\n",
        "# prompt: compare two file\n",
        "# @markdown ###Compares wav files and remove the uncommon utt_ids and transcripts in trans display the removed items\n",
        "# @markdown **Inputs  :** `wav_filepath` list of utt_id.wav and `transcript_filepath` utt_id \\t transcript\n",
        "\n",
        "def compare_and_filter(wav_file, transcript_file, output_file, removed_log_file=None):\n",
        "    \"\"\"\n",
        "    Compares a list of utt_id.wav and a transcript file (utt_id \\t transcript).\n",
        "    Removes and logs transcripts whose utt_ids are not in the wav list.\n",
        "\n",
        "    Args:\n",
        "        wav_file (str): Path to the .wav list file.\n",
        "        transcript_file (str): Path to the transcript file.\n",
        "        output_file (str): Output file for filtered transcripts.\n",
        "        removed_log_file (str, optional): Optional path to write removed items.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(wav_file, 'r') as wav_f:\n",
        "            wav_ids = {line.strip().split('.')[0] for line in wav_f if line.strip()}\n",
        "\n",
        "        removed_items = []\n",
        "\n",
        "        with open(transcript_file, 'r') as trans_f, open(output_file, 'w') as out_f:\n",
        "            for line in trans_f:\n",
        "                if not line.strip():\n",
        "                    continue\n",
        "                parts = line.strip().split('\\t', 1)\n",
        "                if len(parts) != 2:\n",
        "                    print(f\"Skipping malformed line: {line.strip()}\")\n",
        "                    continue\n",
        "                utt_id, transcript = parts\n",
        "                if utt_id in wav_ids:\n",
        "                    out_f.write(line)\n",
        "                else:\n",
        "                    removed_items.append((utt_id, transcript))\n",
        "\n",
        "        print(f\"\\nTotal removed items: {len(removed_items)}\")\n",
        "        for idx, (utt_id, transcript) in enumerate(removed_items, start=1):\n",
        "            print(f\"{idx}. utt_id: {utt_id}, transcript: {transcript}\")\n",
        "\n",
        "        if removed_log_file:\n",
        "            with open(removed_log_file, 'w') as log_f:\n",
        "                for utt_id, transcript in removed_items:\n",
        "                    log_f.write(f\"{utt_id}\\t{transcript}\\n\")\n",
        "\n",
        "    except FileNotFoundError as fnf_err:\n",
        "        print(f\"File not found: {fnf_err}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "wav_filepath = \"/content/wav.txt\"  # @param {\"type\":\"string\",\"placeholder\":\"wavfiles list\"}\n",
        "transcript_filepath = \"/content/text_all\"  # @param{\"type\":\"string\",\"placeholder\":\"text file\"}\n",
        "output_filepath = \"/content/drive/Shareddrives/Audio_ processing/Segmented_data/Punitha_FPU/text\"  # @param{\"type\":\"string\",\"placeholder\":\"path for the output file\"}\n",
        "log_file = \"/content/removed_items.log\"  # @param{\"type\":\"string\",\"placeholder\":\"path for the log file\"}\n",
        "compare_and_filter(wav_filepath, transcript_filepath, output_filepath,log_file)\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "eIX78VXKeVsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cross valdidate  the transcripts and wavs\n",
        "def find_missing_transcripts(wav_file, transcript_file, missing_output_file=None):\n",
        "    \"\"\"\n",
        "    Identifies wav entries whose utt_ids are missing from the transcript file.\n",
        "\n",
        "    Args:\n",
        "        wav_file (str): Path to wav.txt (contains utt_id.wav)\n",
        "        transcript_file (str): Path to transcript file (contains utt_id \\t transcript)\n",
        "        missing_output_file (str, optional): Path to save missing utt_ids. Default is None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(wav_file, 'r') as wf:\n",
        "            wav_ids = {line.strip().split('.')[0] for line in wf if line.strip()}\n",
        "\n",
        "        with open(transcript_file, 'r') as tf:\n",
        "            transcript_ids = {line.strip().split('\\t')[0] for line in tf if '\\t' in line.strip()}\n",
        "\n",
        "        missing_utt_ids = sorted(wav_ids - transcript_ids)\n",
        "\n",
        "        # Output\n",
        "        print(f\"\\nTotal missing transcripts (present in wav.txt but not in transcript): {len(missing_utt_ids)}\")\n",
        "        for idx, utt_id in enumerate(missing_utt_ids, 1):\n",
        "            print(f\"{idx}. utt_id: {utt_id}\")\n",
        "\n",
        "        if missing_output_file:\n",
        "            with open(missing_output_file, 'w') as out_f:\n",
        "                for utt_id in missing_utt_ids:\n",
        "                    out_f.write(f\"{utt_id}.wav\\n\")  # if you want .wav extension\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"File not found: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "find_missing_transcripts(\n",
        "    wav_file=\"/content/wav.txt\", # @param{\"type\":\"string\"}\n",
        "    transcript_file=\"/content/drive/Shareddrives/Audio_ processing/Segmented_data/Punitha_FPU/text\", # @param{\"type\":\"string\"}\n",
        "    missing_output_file=\"/content/missing_utt_ids.txt\" # @param{\"type\":\"string\"}\n",
        ")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "v6bLAucMoIhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Split text into Train and Test\n",
        "import os\n",
        "\n",
        "def split_data(input_file, train_output, test_output, test_ratio=0.2):\n",
        "  \"\"\"\n",
        "  Splits the data in the input file into training and testing sets.\n",
        "\n",
        "  Args:\n",
        "    input_file: Path to the inputfile containing the data.\n",
        "    train_output: Path to the output file for the training data.\n",
        "    test_output: Path to the output file for the testing data.\n",
        "    test_ratio: The ratio of data to be used for testing (default is 0.2)\n",
        "  \"\"\"\n",
        "  os.makedirs(os.path.dirname(train_output), exist_ok=True)\n",
        "  os.makedirs(os.path.dirname(test_output), exist_ok=True)\n",
        "  try:\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
        "      lines = infile.readlines()\n",
        "      total_lines = len(lines)\n",
        "      test_size = int(total_lines * test_ratio)\n",
        "      train_size = total_lines - test_size\n",
        "\n",
        "      # Split the data into training and testing sets\n",
        "      train_data = lines[:train_size]\n",
        "      test_data = lines[train_size:]\n",
        "\n",
        "      # write lines\n",
        "      with open(train_output, 'w', encoding='utf-8') as train_file:\n",
        "        for line in train_data:\n",
        "          train_file.write(line)\n",
        "      with open(test_output, 'w', encoding='utf-8') as test_file:\n",
        "        for line in test_data:\n",
        "          test_file.write(line)\n",
        "    print(\"Data splitted successfully\")\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: Input file '{input_file}' not found.\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Execute\n",
        "input_file = \"/content/text_rm\" #@param{\"type\":\"string\",\"placeholder\":\"Path to spk_2 utt.txt files\"}\n",
        "train_output = \"/content/train/text\" #@param{\"type\":\"string\",\"placeholder\":\"Path to save train.txt files\"}\n",
        "test_output = \"/content/test/text\" #@param{\"type\":\"string\",\"placeholder\":\"Path to save test.txt files\"}\n",
        "split_data(input_file, train_output, test_output)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_dTs8Qf5__FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate  utt, spk, trans, utt2spk, spk2utt files\n",
        "\n",
        "```\n",
        "asr_folder/\n",
        "├── data/\n",
        "│   ├── train/\n",
        "│   │   ├── wav.scp\n",
        "│   │   ├── spk\n",
        "│   │   ├── utt\n",
        "│   │   ├── spk2utt\n",
        "│   │   ├── utt2spk\n",
        "│   │   ├── trans\n",
        "│   │   └── text\n",
        "│   ├── test/\n",
        "│   │   ├── wav.scp\n",
        "│   │   ├── spk\n",
        "│   │   ├── utt\n",
        "│   │   ├── spk2utt\n",
        "│   │   ├── utt2spk\n",
        "│   │   ├── trans\n",
        "│   │   └── text\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "UZDA1TDxh0Rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create spk, utt, trans files from text\n",
        "def create_spk_utt_trans(input_file, output_path):\n",
        "  \"\"\"\n",
        "  Creates spk, utt, spk2utt, trans files from a text file.\n",
        "\n",
        "  Args:\n",
        "    input_file: Path to the input text file.\n",
        "    output_path: Base path for the output files.\n",
        "  \"\"\"\n",
        "\n",
        "  # Get spk, utt_id, trans from text file\n",
        "  with open(input_file, 'r', encoding='utf-8') as infile, open(f\"{output_path}/trans\",'w') as trans, open(f\"{output_path}/spk\",'w') as spk, open(f\"{output_path}/utt\",'w') as utt:\n",
        "    for line in infile:\n",
        "      utt_id, transcript = line.strip().split('\\t')\n",
        "      spk_id = utt_id.split('_')[0]\n",
        "\n",
        "      spk.write(f\"{spk_id}\\n\")\n",
        "      utt.write(f\"{utt_id}\\n\")\n",
        "      trans.write(f\"{transcript}\\n\")\n",
        "  print(\"spk, utt, trans files created\")\n",
        "\n",
        "# Execute\n",
        "text_input = \"/content/test/text\" # @param{\"type\":\"string\",\"placeholder\":\"Path to text file\"}\n",
        "output_path = \"/content/test/\" # @param{\"type\":\"string\",\"placeholder\":\"Path to save spk, utt, trans files\"}\n",
        "create_spk_utt_trans(text_input, output_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IoCy-l0hGBLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create utt2spk from text\n",
        "\n",
        "def create_utt2spk(input_file, output_file):\n",
        "  \"\"\"\n",
        "  Processes a text file containing sentence numbers and transcripts,\n",
        "  cleans the transcripts, and creates a spk2utt.txt file.\n",
        "  input_file: Path to the text file.\n",
        "  output_file: Path to the output spk2utt.txt file.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    out_dir = os.path.dirname(output_file)\n",
        "    if not os.path.exists(out_dir):\n",
        "      os.makedirs(out_dir)\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "      for line in infile:\n",
        "        parts = line.strip().split('\\t', 1)\n",
        "        if len(parts) == 2:\n",
        "          utt_id = parts[0].strip()\n",
        "          transcript = parts[1].strip()\n",
        "\n",
        "          # Cut spk_Id from utt_id\n",
        "          spk_id = utt_id.split('_')[0]\n",
        "\n",
        "\n",
        "          # Store the spk and utt id\n",
        "          outfile.write(f\"{utt_id}\\t{spk_id}\\n\")\n",
        "      print(\"utt2spk_file created\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "      print(f\"Error: Input file '{input_file}' not found.\")\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Provide inputs\n",
        "text_input=\"/content/train/text\" # @param{\"type\":\"string\",\"placeholder\":\"Path to text file\"}\n",
        "output_file=\"/content/train/utt2spk\" # @param{\"type\":\"string\",\"placeholder\":\"Path to save uttrances\"}\n",
        "create_utt2spk(text_input, output_file)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Y8K6MGCzSEWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create spk2utt from utt2spk\n",
        "\n",
        "def create_spk2utt(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Creates a spk2utt file from a utt2spk file.\n",
        "\n",
        "    Args:\n",
        "        input_file: Path to the input utt2spk file (format: utt_id<tab>spk_id)\n",
        "        output_file: Path to the output spk2utt file\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "\n",
        "    try:\n",
        "        spk2utts = defaultdict(list)\n",
        "\n",
        "        with open(input_file, 'r', encoding='utf-8') as infile:\n",
        "            for line in infile:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) != 2:\n",
        "                    continue  # Skip malformed lines\n",
        "                utt_id, spk_id = parts\n",
        "                spk2utts[spk_id].append(utt_id)\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "            for spk_id, utt_list in spk2utts.items():\n",
        "                outfile.write(f\"{spk_id} {' '.join(utt_list)}\\n\")\n",
        "\n",
        "        print(\"spk2utt file created successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file '{input_file}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "# Execute\n",
        "input_file = \"/content/train/utt2spk\" #@param{\"type\":\"string\",\"placeholder\":\"Path to spk_2 utt.txt files\"}\n",
        "output_file = \"/content/train/spk2utt\" #@param{\"type\":\"string\",\"placeholder\":\"Path to save spk2utt files\"}\n",
        "create_spk2utt(input_file, output_file)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WJNzXh5UKmIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title wav.scp\n",
        "\n",
        "def create_wav_scp(input_file, wav_dir, output_file):\n",
        "\n",
        "\n",
        "\n",
        "# path to wav files\n",
        "wav_dir =\"/wavs\" #@param{\"type\":\"string\",\"placeholder\":\"Path to wav files\"}\n",
        "# output file\n",
        "output_file = \"/content/wav.scp\" #@param{\"type\":\"string\",\"placeholder\":\"Path to save wav.scp files\"}"
      ],
      "metadata": {
        "id": "hvXviXv9boBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lexicon generation\n",
        "\n",
        "\n",
        ">  Create ./data/local/dict/lexicon.txt\n",
        "\n",
        "```\n",
        "asr_folder/\n",
        "├── data/\n",
        "│   └── local/\n",
        "│       └── dict/\n",
        "│           └── lexicon.txt\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PHtrIYmjBUJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Unique words Identification\n",
        "\n",
        "import re\n",
        "import os\n",
        "\n",
        "def process_text_files(input_filepath, output_filepath):\n",
        "    \"\"\"\n",
        "    Processes text files to remove unwanted symbols, split into words,\n",
        "    and store unique words in a new text file.\n",
        "    \"\"\"\n",
        "    unique_words = set()\n",
        "\n",
        "    try:\n",
        "        with open(input_filepath, 'r', encoding='utf-8') as infile:\n",
        "            for line in infile:\n",
        "                # Remove unwanted symbols (keep alphanumeric and spaces)\n",
        "                cleaned_line = re.sub(r'[^\\w\\s]', '', line).lower()  # Convert to lowercase\n",
        "                # Remove symbols and digits\n",
        "                cleaned_line = re.sub(r'[^a-zA-Z\\s]', '', line)\n",
        "\n",
        "                # Split the line into words\n",
        "                words = cleaned_line.split()\n",
        "\n",
        "                # Add unique words to the set\n",
        "                unique_words.update(words)\n",
        "\n",
        "        # Write unique words to the output file\n",
        "        with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
        "            for word in sorted(unique_words):  # Sort for better readability\n",
        "                outfile.write(word + '\\n')\n",
        "\n",
        "        print(f\"Unique words saved to: {output_filepath}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file not found at {input_filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage:  Replace with your actual file paths\n",
        "input_file = \"/content/transctipts_FPU(1).txt\"                      # @param{\"type\":\"string\",\"placeholder\": \"Input file containing transcripts\"}\n",
        "output_file = \"/content/drive/Shareddrives/Audio_ processing/Segmented_data/Punitha_FPU/unique_words.txt\"                    # @param{\"type\":\"string\",\"placeholder\":\"Output file to store unique words in transcripts\"}\n",
        "process_text_files(input_file, output_file)"
      ],
      "metadata": {
        "id": "HlxeP4lNo7aR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create lexicon .txt\n",
        "\n",
        "# Phonemes that should stay together\n",
        "grouped_phonemes = [\n",
        "    \"aa\", \"ai\", \"au\", \"dx\", \"ee\", \"eu\", \"ii\", \"lx\", \"nd\", \"ng\", \"nj\", \"nx\",\n",
        "    \"rx\", \"sx\", \"tx\", \"oo\", \"uu\", \"zh\"\n",
        "]\n",
        "\n",
        "# Function to split words into phonemes\n",
        "def split_into_phonemes(word, grouped_phonemes):\n",
        "    phonemes = []\n",
        "    i = 0\n",
        "    while i < len(word):\n",
        "        # Check for grouped phonemes\n",
        "        matched = False\n",
        "        for phoneme in grouped_phonemes:\n",
        "            if word[i:i+len(phoneme)] == phoneme:\n",
        "                phonemes.append(phoneme)\n",
        "                i += len(phoneme)\n",
        "                matched = True\n",
        "                break\n",
        "        # If no grouped phoneme matched, add the single character\n",
        "        if not matched:\n",
        "            phonemes.append(word[i])\n",
        "            i += 1\n",
        "    return phonemes\n",
        "\n",
        "# Input and Output File Paths\n",
        "input_file = \"/content/unique_words.txt\"                       # @param{\"type\":\"string\",\"placeholder\": \"Input file containing words\"}\n",
        "output_file = \"/content/drive/Shareddrives/Audio_ processing/Segmented_data/Punitha_FPU/lexicon.txt\"                           # @param{\"type\":\"string\",\"placeholder\":\"Output file to store word-phoneme mappings\"}\n",
        "\n",
        "# Read words from the input file\n",
        "with open(input_file, \"r\") as infile:\n",
        "    words = [line.strip() for line in infile.readlines()]\n",
        "\n",
        "# Generate phoneme sequences for each word\n",
        "results = []\n",
        "for word in words:\n",
        "    phoneme_sequence = split_into_phonemes(word, grouped_phonemes)\n",
        "    results.append(f\"{word}\\t{' '.join(phoneme_sequence)}\")\n",
        "\n",
        "# Write the results to the output file\n",
        "with open(output_file, \"w\") as outfile:\n",
        "    for line in results:\n",
        "        outfile.write(line + \"\\n\")\n",
        "\n",
        "print(f\"Phoneme sequences written to {output_file}\")\n"
      ],
      "metadata": {
        "id": "Nk8bpJUEuaDF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Merge lexicons\n",
        "\n",
        "# prompt: i have file 1 and file 2 .txt i need to compare  words in file 1 with the 1st word in file 2 and to display the unmatches words in file 1\n",
        "\n",
        "def compare_words(file1_path, file2_path):\n",
        "    \"\"\"\n",
        "    Compares words in file1 with the first word in file2 and displays unmatched words from file1.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file1_path, 'r', encoding='utf-8') as file1, open(file2_path, 'r', encoding='utf-8') as file2:\n",
        "            file1_words = set()\n",
        "            for line in file1:\n",
        "                words = line.strip().split()  # Split each line into words\n",
        "                file1_words.update(words)\n",
        "\n",
        "            first_word_file2 = file2.readline().strip().split()[0] # Get only the 1st word in file2\n",
        "\n",
        "            unmatched_words = file1_words - set([first_word_file2]) #difference between file1 and file2\n",
        "\n",
        "            for word in unmatched_words:\n",
        "                print(word)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: One or both of the files were not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage: Replace with your actual file paths\n",
        "file1_path = \"/content/drive/Shareddrives/Audio_ processing/Segmented_data/MSU/unique_words.txt\"\n",
        "file2_path = \"/content/drive/MyDrive/lexicon.txt\"\n",
        "compare_words(file1_path, file2_path)"
      ],
      "metadata": {
        "id": "Kx0d9uBhla1Z",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio normalization"
      ],
      "metadata": {
        "id": "8eF--apDiqzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "from pydub.effects import normalize\n",
        "\n",
        "def normalize_audio_files(input_dir, output_dir):\n",
        "    \"\"\"\n",
        "    Normalize audio files, including those in subfolders.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): Directory containing input audio files (including subfolders).\n",
        "        output_dir (str): Directory to save normalized audio files, maintaining folder structure.\n",
        "    \"\"\"\n",
        "    for root, dirs, files in os.walk(input_dir):  # Traverse all subdirectories\n",
        "        for filename in files:\n",
        "            if filename.endswith((\".wav\", \".mp3\", \".flac\", \".ogg\")):  # Add other formats if needed\n",
        "                input_path = os.path.join(root, filename)\n",
        "\n",
        "                # Recreate the subfolder structure in the output directory\n",
        "                relative_path = os.path.relpath(root, input_dir)\n",
        "                output_subdir = os.path.join(output_dir, relative_path)\n",
        "                if not os.path.exists(output_subdir):\n",
        "                    os.makedirs(output_subdir)\n",
        "\n",
        "                output_path = os.path.join(output_subdir, filename)\n",
        "\n",
        "                try:\n",
        "                    print(f\"Processing: {input_path}\")\n",
        "\n",
        "                    # Load the audio file\n",
        "                    audio = AudioSegment.from_file(input_path)\n",
        "\n",
        "                    # Normalize the audio\n",
        "                    normalized_audio = normalize(audio)\n",
        "\n",
        "                    # Export the normalized audio to the output directory\n",
        "                    normalized_audio.export(output_path, format=\"wav\")  # Ensure output is in `.wav` format\n",
        "                    print(f\"Saved normalized file to: {output_path}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {input_path}: {e}\")\n",
        "\n",
        "# Set input and output directories\n",
        "input_directory = \"/content/drive/Shareddrives/Audio_ processing/Segmented_data/Suradha_MSR/speech\"\n",
        "output_directory = \"/content/drive/Shareddrives/Audio_ processing/Segmented_data/Suradha_MSR/speech_normalised\"\n",
        "\n",
        "# Run the normalization\n",
        "normalize_audio_files(input_directory, output_directory)\n"
      ],
      "metadata": {
        "id": "A9P7JU9cgS8z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: i have files in sub dirs such as MSU_1_1.wav, MSU_2_1.wav so on i need to rename it as MSR_1_1.wav , MSR_2_1.wav..\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "def rename_files(root_dir):\n",
        "    for subdir, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.startswith(\"MSU_\") and file.endswith(\".wav\"):\n",
        "                old_path = os.path.join(subdir, file)\n",
        "                new_file = file.replace(\"MSU\", \"MSR\", 1)\n",
        "                new_path = os.path.join(subdir, new_file)\n",
        "                os.rename(old_path, new_path)\n",
        "                # print(f\"Renamed '{old_path}' to '{new_path}'\")\n",
        "        print(\"All files renamed successfully.\")\n",
        "\n",
        "# Example usage (replace with your actual directory)\n",
        "root_directory = \"/content/drive/Shareddrives/Audio_ processing/Segmented_data/Suradha_MSR/speech_normalised/\"\n",
        "rename_files(root_directory)"
      ],
      "metadata": {
        "id": "8YQSkfzcgn_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prompt file generation\n",
        "def prompt_generation(input_file, output_path):\n",
        "  with open(input_file, 'r', encoding='utf-8') as infile:\n",
        "    for line in infile:\n",
        "      utt_id, transcript = line.strip().split('\\t')\n",
        "      with open(os.path.join(output_path, f\"{utt_id}.txt\"), 'w', encoding='utf-8') as outfile:\n",
        "        outfile.write(line)\n",
        "    print(\"Prompt files generated\")\n",
        "    except FileNotFoundError:\n",
        "    print(f\"Error: Input file '{input_file}' not found.\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# execution\n",
        "input_file = \"/content/train/text\" #@param{\"type\":\"string\",\"placeholder\":\"Path to text file\"}\n",
        "output_path = \"/content/train/prompts\" #@param{\"type\":\"string\",\"placeholder\":\"Path to save prompt files\"}\n",
        "prompt_generation(input_file, output_path)\n",
        "#"
      ],
      "metadata": {
        "id": "IwOr0IW7nhji"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}